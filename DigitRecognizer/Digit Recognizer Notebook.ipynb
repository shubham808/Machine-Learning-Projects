{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "###*Digit Recognizer*\n\nIn this Notebook we will try to solve the famous Digit Recognition Problem. Its based on the MNIST hand written digits dataset. The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n\nIt is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.  \nLets start!!\n\nWe start by reading the problem description on Kaggle.\n\n###Competition Description\n\nMNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\nIn this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. We’ve curated a set of tutorial-style kernels which cover everything from regression to neural networks. We encourage you to experiment with different algorithms to learn first-hand what works well and how techniques compare.\n\n###Practice Skills\n\n    1. Computer vision fundamentals including simple neural networks\n    2. Classification methods such as SVM and K-nearest neighbors\n\nFrom here we get the idea of what our classifiers may possibly be. We will look at K-neighbors and SVM then compare them to ANNs and later on we will develop a Convolutional neural network.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Make sure you can Import all required libraries before we begin. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score,KFold\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Activation,Flatten\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.utils import np_utils\n\nfrom keras import regularizers",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/opt/conda/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\nUsing TensorFlow backend.\n"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Load Train and Test data**\n============================",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data = pd.read_csv(\"../input/train.csv\").astype('float_')\ntest = pd.read_csv(\"../input/test.csv\").astype('float_')\ntest = test.values\nprint(data.keys())\nprint(data.shape)",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Index(['label', 'pixel0', 'pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5',\n       'pixel6', 'pixel7', 'pixel8',\n       ...\n       'pixel774', 'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779',\n       'pixel780', 'pixel781', 'pixel782', 'pixel783'],\n      dtype='object', length=785)\n(42000, 785)\n"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Seperating Labels and features from Train data and splitting train data for validation",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "labels = data['label']\ndata.drop('label',axis=1,inplace=True)\nfeatures=np.array(data)\nfeatures_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.25,\n                                                                               random_state=42)",
      "execution_count": 3,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Preprocessing the digit images**\n==================================",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Feature Scaling**\n-------------------------------------\n\nImage data consists of pixels which do not have uniformity of distribution around origin we use feature scaling to normalize the data to for better estimation \nIt is used to centre the data around zero mean and unit variance.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "scale = np.max(features_train)\nfeatures_train /= scale\nfeatures_test /= scale\ntest/=scale\nmean = np.std(features_train)\nfeatures_train -= mean\nfeatures_test -= mean\ntest-=mean",
      "execution_count": 4,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now we can use a support vector machine classifier to predict the classes.\nBut it would be better to reduce some features by using principal component analysis",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\npca = PCA(n_components=2).fit(features_train)\nreduced_features_train = pca.transform(features_train)\nreduced_features_test = pca.transform(features_test)\nreduced_test = pca.transform(test)",
      "execution_count": 5,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now we can fit an SVM on these reduced features to validate we will use KFold corss validation",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "clf=SVC()\nclf.fit(reduced_features_train,labels_train)\n#Validating by KFold Cross Validation\nprint(cross_val_score(clf,features,labels,cv=4))",
      "execution_count": 6,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "A Good result but we can do more.\n\n\n1. We could try to use GridsearchCV to Tune the Hyper parameters for better performance\n2. We could use SelectKBest to find the most informative features and train our classifier on them this helps our classifier to not cater to noise.\n3. Use another Classifier ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pipeline = Pipeline(steps=[(\"clf\",SVC())])\n#using grid earchCV on SVM\nparam_grid = [\n  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n ]\n#rbf kernel is default so we as well try with linear too\nclf = GridSearchCV(pipeline, param_grid)\nclf.fit(reduced_features_train,labels_train)\nprint(clf.best_score_)\nprint(clf.best_estimator_)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This does not significantly improve performance Lets try to use the best features from all by using k bests scores",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "k_best.fit(features_train, labels_train)\nscores = k_best.scores_\ni=0\n\nnan_scores = [x for x in scores if x>0]\nnan_scores = np.array(nan_scores)\nprint(min(nan_scores))\n\nfeatures_list=[]\nfor i in range(len(scores)):\n    if scores[i]>min(nan_scores):\n        features_list.append(i)\nprint(features_list)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Using those features you can make a prediction but we found that it does not improve score. Next we will use a simple MLP to model the system. First we have to one-hot-encode the labels ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "*One Hot encoding of labels.*\n-----------------------------\n\nA one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the nth digit will be represented as a vector which is 1 in the nth dimension. \n\nFor example, 3 would be [0,0,0,1,0,0,0,0,0,0].",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\nlabels_train = np_utils.to_categorical(labels_train)\nlabels_test = np_utils.to_categorical(labels_test)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Designing Neural Network Architecture**\n=========================================",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# fix random seed for reproducibility\nseed = 43\nnp.random.seed(seed)\n#Set input dimension\ninput_dim = features_train.shape[1]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "*Sequential Model*\n--------------",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Lets create a simple model from Keras Sequential layer.\n\n1.  In 1st layer of the model we have to define input dimensions of our data in (rows,columns,color channel) format.\n (In theano color channel comes first) but in tesorflow color channel comes at last so make sure you use corect format according to your keras backend\n\n2. Flatten will transform input into 1D array.\n\n3. Dense is fully connected layer that means all neurons in previous layers will be connected to all neurons in fully connected layer.\n In the last layer we have to specify output dimensions/classes of the model.\n Here it's 10, since we have to output 10 different digit labels.\n\n4. Regularization can be used to improve performance its important however to choose a suitable hyperparameter for Norm loss",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "model = Sequential()\nmodel.add(Dense(512, input_dim=input_dim,kernel_regularizer=regularizers.l2(0.0001)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128,kernel_regularizer=regularizers.l2(0.0001)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "***Compile network***\n-------------------\n\nBefore making network ready for training we have to make sure to add below things:\n\n 1.  A loss function: to measure how good the network is\n    \n 2.  An optimizer: to update network as it sees more data and reduce loss\n    value\n    \n 3.  Metrics: to monitor performance of network",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(\"Training...\")\nmodel.fit(features_train, labels_train, validation_split=0.1, verbose=1)\nprint(model.evaluate(features_test,labels_test,batch_size=batch_size,verbose=0))\nprint(\"Generating test predictions...\")\npreds = model.predict_classes(test, verbose=0)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "we do get a pretty good result with this almost 97 percent!\nThats really good\n\n\nBut we can still do better if we use a model specifically designed to deal with image data.\nNext we will use a convolutional neural network because they are better equipped to deal with image features.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "first we will reshape the features into image dimensions currently they are in the form of (len() , image _size* image_size) we will reshape into a form of (len(),image_size,image_size,depth)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\nfeatures_train = features_train.reshape(features_train.shape[0],28,28,1)\nfeatures_test = features_test.reshape(features_test.shape[0],28,28,1)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Next we will define our model.\n\n\nWe will use maxpooling and dropout for regularization. Dropout proabililty is another customizable hyper parameter that can be ueful to tune",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#define our model\nmodel=Sequential()\n#declare input layer use tensorflow backend meaning depth comes at the end\nmodel.add(Convolution2D(32,(3,3),activation='relu',input_shape=(28,28,1)))\nprint(model.output_shape)\n\nmodel.add(Convolution2D(32,(3,3),activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n#conver to 1D by flatten\nmodel.add(Flatten())\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10,activation='softmax'))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nmodel.fit(features_train,labels_train,validation_split=0.1)\nscore = model.evaluate(features_test,labels_test,verbose=0)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "test = test.reshape(test.shape[0],28,28,1)\n\npred = model.predict_classes(test,verbose=0)\n\npred = pd.DataFrame(pred)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "More to come . Please upvote if you find it useful.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "You can increase number of epochs on your local machine to get better results.",
      "metadata": {}
    }
  ]
}